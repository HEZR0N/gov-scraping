{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdPvoW85jvBMwlxXk87TAS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT02xLnLmG_-",
        "outputId": "8dd10caa-f596-425d-bc9b-187710ad57ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Import essential tools.\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Function removes punctuations from topics in a given list.\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "#Function removes stop_words from given list, returns filtered list.\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) \n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "#Creation of stop_words filter used to remove useless words from text.            \n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "keywords_set = set()\n",
        "\n",
        "#LDAModel Function. Input CSV file to recieve a list of keywords.\n",
        "'''!!!!HARD-CODED-REQUIREMENT!!!! â†’ CSV File MUST have 'Title' and 'Abstract' columns/fields.'''\n",
        "def LDAModel(CSV_FILE, num_of_topics, topn_keywords):\n",
        "  #Read in Dataset from CSV file.\n",
        "  dataframe = pd.read_csv(CSV_FILE, encoding='unicode_escape')\n",
        "\n",
        "  #Check for hard coded requirements, raise ValueError for invalid CSV files.\n",
        "  if 'Titles' and 'Abstract' not in dataframe.columns: \n",
        "    raise ValueError(\"No 'Titles' or 'Abstract' fields in CSV file.\")\n",
        "  \n",
        "  #Create Titles and Abstracts Dataframes.\n",
        "  Titles = pd.DataFrame(dataframe.Title)\n",
        "  Abstracts = pd.DataFrame(dataframe.Abstract)\n",
        "\n",
        "  #Text Cleaning & Preprocessing\n",
        "  Titles['Title'] = Titles['Title'].map(lambda x: re.sub('[,/\\.!?]', '', str(x)))\n",
        "  Titles['Title'] = Titles['Title'].map(lambda x: x.lower())\n",
        "\n",
        "  Abstracts['Abstract'] = Abstracts['Abstract'].map(lambda x: re.sub('[,/\\.!?]', '', str(x)))\n",
        "  Abstracts['Abstract'] = Abstracts['Abstract'].map(lambda x: x.lower())\n",
        "\n",
        "  #Clean datasets for LDA Input.\n",
        "  titles_data = Titles['Title'].tolist()\n",
        "  title_words = list(sent_to_words(titles_data))\n",
        "  title_words = remove_stopwords(title_words)\n",
        "\n",
        "  abstracts_data = Abstracts['Abstract'].tolist()\n",
        "  abstracts_words = list(sent_to_words(abstracts_data))\n",
        "  abstract_words = remove_stopwords(abstracts_words)\n",
        "  \n",
        "  #Create Corpora Dictionaries.\n",
        "  title_dict = corpora.Dictionary(title_words)\n",
        "  abstract_dict = corpora.Dictionary(abstract_words)\n",
        "\n",
        "  #Convert dictionaries into bag-of-words format. \n",
        "  title_corpus = [title_dict.doc2bow(text) for text in title_words]\n",
        "  abstract_corpus = [abstract_dict.doc2bow(text) for text in abstract_words]\n",
        "\n",
        "  #Build & instantiate LDA Models\n",
        "  titles_lda = gensim.models.LdaModel(corpus=title_corpus, id2word=title_dict, num_topics=num_of_topics)\n",
        "  abtracts_lda = gensim.models.LdaModel(corpus=abstract_corpus, id2word=abstract_dict, num_topics=num_of_topics)\n",
        "\n",
        "  #Create \"Topn\" dictionaries from LDA model's unsupervised classification.\n",
        "  #Here we use our input parameters \"num_of_topics\" and \"topn_keywords\" to bound our search.\n",
        "  #num_of_topics is how many classifications the model sets for itself.\n",
        "  #topn_keywords is how many words the model associates with each classification.\n",
        "  topn_title_words = {'Topic_' + str(i): [word for word, prob in titles_lda.show_topic(i, topn=topn_keywords)] for i in range(0, titles_lda.num_topics)}\n",
        "  topn_abstract_words = {'Topic_' + str(i): [word for word, prob in abtracts_lda.show_topic(i, topn=topn_keywords)] for i in range(0, abtracts_lda.num_topics)}\n",
        "\n",
        "  #Process keywords associated with each classification, add to keywords_set.\n",
        "  #Titles keywords:\n",
        "  for x, y in topn_title_words.items():\n",
        "    for i in y:\n",
        "      keywords_set.add(i)\n",
        "  #Abstracts keywords:\n",
        "  for x, y in topn_abstract_words.items():\n",
        "    for i in y:\n",
        "      keywords_set.add(i)\n",
        "\n",
        "  #Print clusters found in LDA.\n",
        "  print(topn_title_words)\n",
        "\n",
        "  #Return lsit by converting set.\n",
        "  return list(keywords_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Output topic keywords to textfile.\n",
        "'''with open('CICI_Keywords.txt','w') as tfile:\n",
        "\ttfile.write('\\n'.join(x))'''"
      ],
      "metadata": {
        "id": "Jw4Gwh5hI6r6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f4fed7c-ede1-4af7-9de8-5831378f3da4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"with open('CICI_Keywords.txt','w') as tfile:\\n\\ttfile.write('\\n'.join(x))\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Scan directory for csv files, run LDAModel on dataset, and write to corresponding file.\n",
        "for x in os.listdir(os.getcwd()):\n",
        "  if os.path.isfile(x):\n",
        "    if '.csv' in x:\n",
        "      print(x)\n",
        "      print(type(x))\n",
        "      c = LDAModel(os.path.basename(x), 10, 5)\n",
        "      with open(x.strip('.csv')+'_'+'keywords.txt','w') as tfile:\n",
        "          tfile.write('\\n'.join(c))"
      ],
      "metadata": {
        "id": "0-AxI3ynoPJC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}